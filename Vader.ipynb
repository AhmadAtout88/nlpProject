{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "149f04f6-2de7-4880-a729-d594f19e83f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/ahmedatout/nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ahmedatout/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ahmedatout/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNeutral\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Apply sentiment analysis to the dataset\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(get_sentiment)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Split data into training and test sets for evaluation\u001b[39;00m\n\u001b[1;32m     38\u001b[0m train_data, test_data \u001b[38;5;241m=\u001b[39m train_test_split(data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]], test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[1;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4919\u001b[0m         func,\n\u001b[1;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[1;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[1;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[1;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[1;32m   1509\u001b[0m )\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[1], line 26\u001b[0m, in \u001b[0;36mget_sentiment\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_sentiment\u001b[39m(text):\n\u001b[0;32m---> 26\u001b[0m     sentiment_scores \u001b[38;5;241m=\u001b[39m sia\u001b[38;5;241m.\u001b[39mpolarity_scores(text)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sentiment_scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompound\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m:\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPositive\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/sentiment/vader.py:366\u001b[0m, in \u001b[0;36mSentimentIntensityAnalyzer.polarity_scores\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03mReturn a float for sentiment strength based on the input text.\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;124;03mPositive values are positive valence, negative value are negative\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    matched as if it was a normal word in the sentence.\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;66;03m# text, words_and_emoticons, is_cap_diff = self.preprocess(text)\u001b[39;00m\n\u001b[0;32m--> 366\u001b[0m sentitext \u001b[38;5;241m=\u001b[39m SentiText(\n\u001b[1;32m    367\u001b[0m     text, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstants\u001b[38;5;241m.\u001b[39mPUNC_LIST, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstants\u001b[38;5;241m.\u001b[39mREGEX_REMOVE_PUNCTUATION\n\u001b[1;32m    368\u001b[0m )\n\u001b[1;32m    369\u001b[0m sentiments \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    370\u001b[0m words_and_emoticons \u001b[38;5;241m=\u001b[39m sentitext\u001b[38;5;241m.\u001b[39mwords_and_emoticons\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/sentiment/vader.py:270\u001b[0m, in \u001b[0;36mSentiText.__init__\u001b[0;34m(self, text, punc_list, regex_remove_punctuation)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, punc_list, regex_remove_punctuation):\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 270\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(text\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m=\u001b[39m text\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPUNC_LIST \u001b[38;5;241m=\u001b[39m punc_list\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'all_sentiment_analysis.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Sentiment Analysis\n",
    "# Initialize VADER sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to determine sentiment\n",
    "def get_sentiment(text):\n",
    "    sentiment_scores = sia.polarity_scores(text)\n",
    "    if sentiment_scores['compound'] >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif sentiment_scores['compound'] <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# Apply sentiment analysis to the dataset\n",
    "data['Sentiment'] = data['Text'].apply(get_sentiment)\n",
    "\n",
    "# Split data into training and test sets for evaluation\n",
    "train_data, test_data = train_test_split(data[['Text', 'Sentiment']], test_size=0.2, random_state=42)\n",
    "\n",
    "# Topic Modeling\n",
    "stop_words = set(stopwords.words('english'))\n",
    "texts_processed = [\n",
    "    [word for word in word_tokenize(doc.lower()) if word.isalpha() and word not in stop_words]\n",
    "    for doc in data['Preprocessed_Text'].dropna()\n",
    "]\n",
    "\n",
    "# Create Dictionary and Corpus\n",
    "dictionary = Dictionary(texts_processed)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts_processed]\n",
    "\n",
    "# Build LDA Model\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, random_state=42, passes=10)\n",
    "\n",
    "# Evaluate the model with Coherence Score\n",
    "coherence_model = CoherenceModel(model=lda_model, texts=texts_processed, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "# Display Results\n",
    "print(\"Sample Sentiment Analysis Results:\")\n",
    "print(train_data.head(10))\n",
    "\n",
    "print(\"\\nTopic Modeling Results:\")\n",
    "for idx, topic in lda_model.print_topics(num_words=10):\n",
    "    print(f\"Topic {idx}: {topic}\")\n",
    "\n",
    "print(f\"\\nCoherence Score: {coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee1389eb-7e0c-40ea-9e30-e018854577fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/ahmedatout/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ahmedatout/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ahmedatout/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Sentiment Analysis Results:\n",
      "                                                  Text Sentiment\n",
      "57   Because if like you don't go basically it's li...  Positive\n",
      "228  I remember because it was around Thanksgiving ...   Neutral\n",
      "281  please talk about the impact of stuttering on ...   Neutral\n",
      "486  After the group was over, I gave her some advi...  Positive\n",
      "221                                        go for it .   Neutral\n",
      "448  is there anything you or what else would you w...  Positive\n",
      "422                              as best as possible .  Positive\n",
      "376  so with family (.) they don't really know that...  Negative\n",
      "25                I would graduate some better school.  Positive\n",
      "230  so she specialized in stuttering but she wasn'...  Negative\n",
      "\n",
      "Topic Modeling Results:\n",
      "Topic 0: 0.062*\"l\" + 0.031*\"thing\" + 0.015*\"kind\" + 0.014*\"work\" + 0.012*\"n\" + 0.012*\"important\" + 0.009*\"xxx\" + 0.009*\"hand\" + 0.009*\"acceptance\" + 0.009*\"go\"\n",
      "Topic 1: 0.069*\"stutter\" + 0.042*\"like\" + 0.039*\"people\" + 0.030*\"life\" + 0.025*\"talk\" + 0.021*\"stuttering\" + 0.021*\"impact\" + 0.017*\"different\" + 0.017*\"think\" + 0.016*\"support\"\n",
      "Topic 2: 0.025*\"like\" + 0.025*\"good\" + 0.022*\"year\" + 0.020*\"think\" + 0.019*\"stutter\" + 0.017*\"speech\" + 0.016*\"oh\" + 0.016*\"old\" + 0.016*\"thing\" + 0.014*\"cool\"\n",
      "Topic 3: 0.051*\"stuttering\" + 0.043*\"therapy\" + 0.034*\"experience\" + 0.034*\"think\" + 0.033*\"school\" + 0.026*\"cause\" + 0.023*\"mean\" + 0.023*\"yeah\" + 0.022*\"communication\" + 0.022*\"successful\"\n",
      "Topic 4: 0.034*\"know\" + 0.026*\"stutter\" + 0.024*\"like\" + 0.022*\"way\" + 0.020*\"therapist\" + 0.018*\"speech\" + 0.016*\"want\" + 0.014*\"school\" + 0.012*\"day\" + 0.012*\"express\"\n",
      "\n",
      "Coherence Score: 0.4666431985828054\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'all_sentiment_analysis.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop rows with missing or invalid text\n",
    "data = data.dropna(subset=['Text'])\n",
    "data['Text'] = data['Text'].astype(str)\n",
    "\n",
    "# Sentiment Analysis\n",
    "# Initialize VADER sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to determine sentiment\n",
    "def get_sentiment(text):\n",
    "    sentiment_scores = sia.polarity_scores(text)\n",
    "    if sentiment_scores['compound'] >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif sentiment_scores['compound'] <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# Apply sentiment analysis to the dataset\n",
    "data['Sentiment'] = data['Text'].apply(get_sentiment)\n",
    "\n",
    "# Split data into training and test sets for evaluation\n",
    "train_data, test_data = train_test_split(data[['Text', 'Sentiment']], test_size=0.2, random_state=42)\n",
    "\n",
    "# Topic Modeling\n",
    "stop_words = set(stopwords.words('english'))\n",
    "texts_processed = [\n",
    "    [word for word in word_tokenize(doc.lower()) if word.isalpha() and word not in stop_words]\n",
    "    for doc in data['Preprocessed_Text'].dropna()\n",
    "]\n",
    "\n",
    "# Create Dictionary and Corpus\n",
    "dictionary = Dictionary(texts_processed)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts_processed]\n",
    "\n",
    "# Build LDA Model\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, random_state=42, passes=10)\n",
    "\n",
    "# Evaluate the model with Coherence Score\n",
    "coherence_model = CoherenceModel(model=lda_model, texts=texts_processed, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "# Display Results\n",
    "print(\"Sample Sentiment Analysis Results:\")\n",
    "print(train_data.head(10))\n",
    "\n",
    "print(\"\\nTopic Modeling Results:\")\n",
    "for idx, topic in lda_model.print_topics(num_words=10):\n",
    "    print(f\"Topic {idx}: {topic}\")\n",
    "\n",
    "print(f\"\\nCoherence Score: {coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ff98c7-70e4-4c08-8f07-5fc68820255c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
